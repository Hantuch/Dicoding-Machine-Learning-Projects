# -*- coding: utf-8 -*-
"""ML_Pengembangan_NLP_Submission_Hanif_Rachmadani

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vww2jeTecVG4s_jCksiGJNUZTOa_ZjFy
"""

# Dicoding Course "Belajar Pengembangan Machine Learning" 1st Project: NLP

# News Category Classifier

# Hanif Rachmadani on 14/07/2021.

# Dataset used : https://www.kaggle.com/hgultekin/bbcnewsarchive

# TF & Data-prepping Libraries Import

from google.colab import files

import tensorflow as tf
import pandas as pd

from sklearn.model_selection import train_test_split

# Dataset Download

files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d hgultekin/bbcnewsarchive

!mkdir bbcnewsarchive
!unzip bbcnewsarchive.zip -d bbcnewsarchive
!ls bbcnewsarchive

# Initial Dataframe Loading

df_initial = pd.read_csv('bbcnewsarchive/bbc-news-data.csv', sep='\t')

df_initial.head()

# Dataframe Prepping

emotion = pd.get_dummies(df_initial.category)
df_new = pd.concat([df_initial,emotion],axis=1)

df_new['title'] = df_new['title'] + df_new['content']

df_new.rename(columns={'title':'fulltext'},inplace='true')
df_new.drop(columns=['filename','content','category'], inplace=True)

df_new.head()

# Lemmatizer & Stopwords Libraries Import

import nltk
import numpy as np
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer

# Stop Words Filter

def filter_stop_words(train_sentences, stop_words):
    for i, sentence in enumerate(train_sentences):
        new_sent = [word for word in sentence if word not in stop_words]
        train_sentences[i] = ' '.join(new_sent)
    return train_sentences

# Stop Words Filtering & Word Lemmatizing

stop_words = stopwords.words('english')

def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(w,pos='v') for w in text] 

text = df_new['fulltext'].apply(nltk.word_tokenize).apply(lemmatize_text)
text = filter_stop_words(text, stop_words)

# Train Test Split

text = text.values
label = df_new[['business','entertainment','politics','sport','tech']].values

text_train, text_val, label_train, label_val = train_test_split(text, label, test_size=0.2)   # Validation Data 20% Total

print(text_train)
print(label_train)

# Data Tokenizing, Sequencing, & Padding

max_words = 1000      # Max Tokenized Words
sent_length = 300     # Max Sentence Length
trunc_type = "post"   # Truncation Type

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=max_words, oov_token='oov', filters='"#$%&()*+,-./:;<=>@[\\]^_`{|}~\t\n1234567890') 

tokenizer.fit_on_texts(text_train) 
tokenizer.fit_on_texts(text_val)

train_seq = tokenizer.texts_to_sequences(text_train)
val_seq = tokenizer.texts_to_sequences(text_val )
 
padded_train = pad_sequences(train_seq, truncating=trunc_type, maxlen=sent_length) 
padded_val = pad_sequences(val_seq, truncating=trunc_type, maxlen=sent_length)

# Model Design

# 1 Embedding Layer, 1 LSTM Layer, 2 Hidden Layer, 1 Output Layer

model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(input_dim=max_words, output_dim=64, input_length=sent_length),  
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')                                
])

# Model Compilation

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.summary()

# Callback Function Declaration

class modelCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):  
      print("Accuracy Target of >90% on Both Datasets Reached, Stopping Training...")
      self.model.stop_training = True

# Model Training

history = model.fit(
            padded_train,
            label_train,
            batch_size=32,                            # 32 Data per Batch Loading
            steps_per_epoch=50,                       # 50 Batches Used per Epoch for Training
            epochs=50,                                # 50 Maximum Epoch (Arbitratry)
            validation_data=(padded_val,label_val),     
            validation_steps=10,                      # 10 Batches Used per Epoch for Validation
            callbacks=[modelCallback()],              # Callback
            verbose='auto')

# Pyplot Import

import matplotlib.pyplot as plt

# Loss & Accuracy Graph

plt.plot(history.history['loss'], color="orange")
plt.plot(history.history['val_loss'], color="red")
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train','Validation'], loc='upper right')
plt.show()

plt.plot(history.history['accuracy'], color="cyan")
plt.plot(history.history['val_accuracy'], color="blue")
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train','validation'], loc='lower right')
plt.show()